
"""
Assume there are n numbers of time series generated by various sensor probes.
Each time series has numerial values with the same time interval.
Assume the size of the remaining data is much bigger than that of the missing data.

Design a deep learning model so as to learn the temporal relationships
    among multiple time indexes and the n monitoring parameters.

SSIM: sequence-to-sequence imputation model
    with the attention mechanism
    designed to accept and output variable-length data sequences
Variable-length sliding window algorithm
"""

import pdb
import os, inspect, time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
from torch.nn.utils.rnn import pack_padded_sequence
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt

import Network
import dataUtil


modelname = 'model'
resultname = 'result'
summaryname = 'tensorboard'

tr_dataname = ['ssim_tr_in_pack_intel44.pt',
               'ssim_tr_out_pack_intel44.pt',
               'ssim_tr_len_pack_intel44.pt']
# te_data = []

input_size = 1
enc_hid_size = 25
dec_hid_size = 25
output_size = 1

learning_rate = 0.001
batch_size = 100

epoch_last = -1
epoch_max = 500

''' -------- '''
''' SET PATH '''
''' -------- '''
current_path = inspect.getfile(inspect.currentframe())  # main.py
current_dir = os.path.dirname(os.path.abspath(current_path))  # ssim
DATA_PATH = os.path.join(os.path.dirname(current_dir), 'data')  # lstm\data
MODEL_PATH = os.path.join(current_dir, modelname)
RESULT_PATH = os.path.join(current_dir, resultname)
SUMM_PATH = os.path.join(current_dir, summaryname)

def check_path(path):
    if not os.path.exists(path):
        os.mkdir(path)
check_path(DATA_PATH)
check_path(MODEL_PATH)
check_path(RESULT_PATH)
check_path(SUMM_PATH)


''' ----------- '''
''' Tensorboard '''
''' ----------- '''
"""
execution:
tensorboard --logdir=./SUMM_PATH
"""
summary = SummaryWriter(SUMM_PATH)  # Writer will output to ./runs/ directory by default


''' ---------------------------------------- '''
''' Load Data & Prepare Dataset (Train/Test) '''
''' ---------------------------------------- '''
dataUtil = dataUtil.Data(DATA_PATH)

train_input = dataUtil.load_data(tr_dataname[0])
train_target = dataUtil.load_data(tr_dataname[1])
train_lengths = dataUtil.load_data(tr_dataname[2])
print('\tLoad Data')
print('\t\tNb. of train data: ', train_input.shape[0])

#################################################### test 하는 부분 만들고, (on the fly test)
#################################################### cross validation 만들고,
#################################################### 자잘자잘한 부분 정리하기!!

# deletion method for train_input/ train_target
nan_r_idx = np.unique(np.concatenate((np.where(np.isnan(train_input) == True)[0],
                            np.where(np.isnan(train_target) == True)[0])))
nan_r_idx[::-1].sort()  # for delete rows

print('\tWarning!!')
print('\t\t{} rows with NaN data detected in train data'.format(nan_r_idx.shape[0]))

for r_idx in nan_r_idx:
    # print(r_idx)
    train_input = np.delete(train_input, r_idx, axis=0)  # delete row
    train_target = np.delete(train_target, r_idx, axis=0)  # delete row
    train_lengths = np.delete(train_lengths, r_idx)
print('\n\tNaN data deleted')
print('\t\tNb. of train data: ', train_input.shape[0])

train_input = torch.from_numpy(train_input)
train_target = torch.from_numpy(train_target)
train_lengths = torch.from_numpy(train_lengths)

train_input = train_input.float()
train_target = train_target.float()

# train_packed_input = nn.utils.rnn.pack_padded_sequence(train_input, train_lengths.tolist(), batch_first=True)

train_data_loader = Data.TensorDataset(train_input, train_target)
train_loader = Data.DataLoader(dataset=train_data_loader, batch_size=batch_size, shuffle=True)

# test_input =
# test_target =

## cross validation도 torch안에 기능이 있을거 같은데, 찾아보기


''' ----------- '''
''' Build model '''
''' ----------- '''
net = Network.SSIM(input_size, enc_hid_size, dec_hid_size, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=learning_rate)


# ''' --- '''
# ''' GPU '''
# ''' --- '''
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# print("Message: use ", device)
# net.to(device)


''' ------------ '''
''' Train & Test '''
''' ------------ '''
epoch_start = epoch_last + 1

print('\n\n""" ----------------------- """')
print('"""      Start Training     """')
print('""" ----------------------- """')
print('    learning rate = %.5f'%learning_rate)
print('    batch size    = %7d'%batch_size)
print('    start at epoch %d'%epoch_start)
print('""" ----------------------- """\n')

for i in range(epoch_start, epoch_max):
    print('STEP: ', i)

    ''' train with mini-batch '''
    for j, (input_mini, target_mini) in enumerate(train_loader):
        # print(j)
        # continue

        # print('\tminibatch %d'%j)


        # if j == 21:
            # pdb.set_trace()

        # get length information
        input_mini_lengths = np.where(input_mini != 0)[1]  # column infos.
        temp_idx = np.where(input_mini_lengths == 0)
        temp_idx = temp_idx[0][1:] - 1
        input_mini_lengths = np.concatenate((input_mini_lengths[temp_idx] + 1, [input_mini_lengths[-1] + 1]))

        # sort
        sorted_idx = np.argsort(input_mini_lengths)[::-1]  # descending order
        input_mini = torch.from_numpy(input_mini.detach().numpy()[sorted_idx])
        input_mini_lengths = input_mini_lengths[sorted_idx]
        target_mini = torch.from_numpy(target_mini.detach().numpy()[sorted_idx])

        # pdb.set_trace()

        # get packed sequence
        packed_input_mini = pack_padded_sequence(input_mini.unsqueeze(2), input_mini_lengths.tolist(), batch_first=True)

        out = net(packed_input_mini)
        # out = net(input_mini.unsqueeze(2))  # 일단, 어거지로 (batch, seq_len, input_size)로 맞춤, 나중에 input_size가 달라지면 여기 다시 봐야함...
        # pdb.set_trace()

        for output_idx, output_length in enumerate(input_mini_lengths):
            out[output_idx][output_length:] = 0

        # try:
        loss = criterion(out, target_mini[:, :out.size(1)])
        # except:
            # pdb.set_trace()

        summary.add_scalar('train_loss(mini_batch)', loss.item(), (284*i)+j)
        print('Epoch {} minibatch {} Loss : {}'.format(i, j, loss.item()))

        # pdb.set_trace()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


        if j%100 == 0:
            torch.save(net.state_dict(), os.path.join(MODEL_PATH, 'model_epoch%d_batch%d.pth'%(i, j)))
            print('\t\tmodel at iteration %d saved'%i)

    # if i%10 == 0:
        # print('Epoch {} Loss : {}'.format(i, loss.item()))

    # torch.save(seq.state_dict(), os.path.join(MODEL_PATH, 'model_epoch%d.pth'%i))
    # print('\t\tmodel at iteration %d saved'%i)

pdb.set_trace()
