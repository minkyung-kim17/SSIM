
"""
Assume there are n numbers of time series generated by various sensor probes.
Each time series has numerial values with the same time interval.

Assume the size of the remaining data is much bigger than that of the missing data.

Design a deep learning model so as to learn the temporal relationships
    among multiple time indexes and the n monitoring parameters. (근데 아직 1)

SSIM: sequence-to-sequence imputation model with the attention mechanism
    designed to accept and output variable-length data sequences
Variable-length sliding window algorithm
"""

import pdb
import os, inspect, time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
from torch.nn.utils.rnn import pack_padded_sequence
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
import pickle

import Network
import dataUtil

### + cross validation
### + other issues (reload model, visualization, ...)
### + expand single time series data to multivariate time series data

''' -------------- '''
''' USER PARAMETER '''
''' -------------- '''
# te_left_len_min = 2
# te_left_len_max = 7
# te_right_len_min = 2
# te_right_len_max = 7
missing_length = 5

input_size = 1
enc_hid_size = 25
dec_hid_size = 25
output_size = 1

learning_rate = 0.001
batch_size = 100

epoch_last = -1
epoch_max = 500


''' -------- '''
''' SET PATH '''
''' -------- '''
modelname = 'model'
resultname = 'result'
summaryname = 'tensorboard'

current_path = inspect.getfile(inspect.currentframe())  # main.py
current_dir = os.path.dirname(os.path.abspath(current_path))  # ssim
DATA_PATH = os.path.join(os.path.dirname(current_dir), 'data')  # lstm\data
MODEL_PATH = os.path.join(current_dir, modelname)
RESULT_PATH = os.path.join(current_dir, resultname)
SUMM_PATH = os.path.join(current_dir, summaryname)

def check_path(path):
    if not os.path.exists(path):
        os.mkdir(path)
check_path(DATA_PATH)
check_path(MODEL_PATH)
check_path(RESULT_PATH)
check_path(SUMM_PATH)


''' ----------- '''
''' Tensorboard '''
''' ----------- '''
"""
execution:
tensorboard --logdir=./SUMM_PATH
"""
summary = SummaryWriter(SUMM_PATH)  # Writer will output to ./runs/ directory by default


''' ---------------------------------------- '''
''' Load Data & Prepare Dataset (Train/Test) '''
''' ---------------------------------------- '''
# tr/te datafile names (made at dataUtils.py)
tr_datanames = ['ssim_tr_in_pack_intel44.pt',
               'ssim_tr_len_pack_intel44.pt',
               'ssim_tr_out_pack_intel44.pt']  # in alphabetical order

te_datanames = {}  # key: values; (len_left, len_right): filenames (in, len, out)
te_datanames_temp = [filename for filename in os.listdir(DATA_PATH) if filename.find('ssim_te') is not -1]
for filename in te_datanames_temp:
    temp = filename.split('.')[0].split('_')
    try:
        len_left, len_right = int(temp[-2]), int(temp[-1])
    except:
        continue  # filename 'ssim_te_in_pack_intel44'

    if (len_left, len_right) in te_datanames.keys():
        te_datanames[(len_left, len_right)] += [filename]
    else:
        te_datanames[(len_left, len_right)] = [filename]

# preprocessing tr/te data
dataUtil = dataUtil.Data(DATA_PATH)

print('======TRAIN DATA=====')
print('LOAD DATA:')
train_input = dataUtil.load_data(tr_datanames[0])
train_length = dataUtil.load_data(tr_datanames[1])
train_target = dataUtil.load_data(tr_datanames[2])
print('\t{} \t{} \t{}'.format(tr_datanames[0], tr_datanames[1], tr_datanames[2]))
print('\tNb. of train data: ', train_input.shape[0])

train_input, train_target, train_length = dataUtil.preprocessing(train_input,
                                                                 train_target,
                                                                 train_length)
train_data_loader = Data.TensorDataset(train_input, train_target)
train_loader = Data.DataLoader(dataset=train_data_loader, batch_size=batch_size, shuffle=True)

print('======TEST DATA=====')
test_inputs = {}
test_targets = {}
test_lengths = {}
for key in te_datanames.keys():
    print('LOAD DATA:')
    te_dataname = te_datanames[key]
    test_input = dataUtil.load_data(te_dataname[0])
    test_length = dataUtil.load_data(te_dataname[1])
    test_target = dataUtil.load_data(te_dataname[2])
    print('\t{} \t{} \t{}'.format(te_dataname[0], te_dataname[1], te_dataname[2]))
    print('\tNb. of train data: ', test_input.shape[0])

    test_input, test_target, test_length = dataUtil.preprocessing(test_input,
                                                                   test_target,
                                                                   test_length)
    test_inputs[key] = test_input
    test_targets[key] = test_target
    test_lengths[key] = test_length

# with open('ssim_te_in_pack_intel44.pickle', 'wb') as f:
#     pickle.dump(test_inputs, f, protocol=pickle.HIGHEST_PROTOCOL)
# with open('ssim_te_out_pack_intel44.pickle', 'wb') as f:
#     pickle.dump(test_targets, f, protocol=pickle.HIGHEST_PROTOCOL)
# with open('ssim_te_len_pack_intel44.pickle', 'wb') as f:
#     pickle.dump(test_lengths, f, protocol=pickle.HIGHEST_PROTOCOL)


''' ----------- '''
''' Build model '''
''' ----------- '''
net = Network.SSIM(input_size, enc_hid_size, dec_hid_size, output_size)
criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=learning_rate)


# ''' --- '''
# ''' GPU '''
# ''' --- '''
# device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# print("Message: use ", device)
# net.to(device)


''' ------------ '''
''' Train & Test '''
''' ------------ '''
epoch_start = epoch_last + 1

print('\n\n')
print('""" ----------------------- """')
print('"""      Start Training     """')
print('""" ----------------------- """')
print('    learning rate = %.5f'%learning_rate)
print('    batch size    = %7d'%batch_size)
print('    start at epoch %d'%epoch_start)
print('""" ----------------------- """\n')

for i in range(epoch_start, epoch_max):
    ''' Train with mini-batch '''
    print('STEP: ', i)
    for j, (input_mini, target_mini) in enumerate(train_loader):
        ''' to make packed sequence with various length of training data '''
        # get length information
        input_mini_lengths = np.where(input_mini != 0)[1]  # column infos.
        temp_idx = np.where(input_mini_lengths == 0)
        temp_idx = temp_idx[0][1:] - 1
        input_mini_lengths = np.concatenate((input_mini_lengths[temp_idx] + 1, [input_mini_lengths[-1] + 1]))

        # sort
        sorted_idx = np.argsort(input_mini_lengths)[::-1]  # descending order
        input_mini = torch.from_numpy(input_mini.detach().numpy()[sorted_idx])
        input_mini_lengths = input_mini_lengths[sorted_idx]
        target_mini = torch.from_numpy(target_mini.detach().numpy()[sorted_idx])

        # get packed sequence
        packed_input_mini = pack_padded_sequence(input_mini.unsqueeze(2), input_mini_lengths.tolist(), batch_first=True)
        # 나중에 multivariate sequence가 되면, 여기 unsqueeze(2) 부분 다시 확인해야함!

        out = net(packed_input_mini)

        for output_idx, output_length in enumerate(input_mini_lengths):
            out[output_idx][output_length:] = 0

        loss = criterion(out, target_mini[:, :out.size(1)])

        summary.add_scalar('train_loss(mini_batch)', loss.item(), (284*i)+j)
        print('Epoch {} minibatch {} Loss : {}'.format(i, j, loss.item()))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if j%100 == 0:
            torch.save(net.state_dict(), os.path.join(MODEL_PATH, 'model_epoch%d_batch%d.pth'%(i, j)))
            print('\tmodel at iteration %d saved'%i)

            ''' Test '''
            with torch.no_grad():
                for key in test_inputs.keys():
                    test_input, test_target = test_inputs[key], test_targets[key]

                    test_out = net(test_input.unsqueeze(2))

                    test_loss = criterion(test_out, test_target)

                    print('\t{}:: test loss: {}'.format(key, test_loss.item()))

                    ''' missing value imputation '''
                    missing_out = test_out[:, key[0]:key[0]+missing_length]
                    missing_target = test_target[:, key[0]:key[0]+missing_length]

                    mape = torch.mean(torch.mean(abs((missing_out - missing_target) / missing_target), dim=1)*100)
                    mae = torch.mean(torch.mean(abs(missing_out - missing_target), dim=1))
                    # print('\t\ttest mae: ', mae.item())
                    # print('\t\ttest mae: ', mape.item())

                summary.add_scalar('test_loss {}'.format(key), test_loss.item(), (284*i)+j)
                summary.add_scalar('mape {}'.format(key), mape.item(), (284*i)+j)
                summary.add_scalar('mae {}'.format(key), mae.item(), (284*i)+j)

pdb.set_trace()
